import sklearn as sklearn
import pandas as pd
import xgboost as xgb
import seaborn as sns
import lightgbm as lgbm
import matplotlib as plt
import shap as shap
# =====
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

demo = pd.read_csv("demographic.csv")
labs = pd.read_csv("labs.csv")
diet = pd.read_csv("diet.csv")
questions = pd.read_csv("questionnaire.csv")
examination = pd.read_csv("examination.csv")

# merging five main data files 
df = demo \
    .merge(labs, on="SEQN", how="left") \
    .merge(diet, on="SEQN", how="left") \
    .merge(questions, on="SEQN", how="left") \
    .merge(examination, on="SEQN", how="left")

# cleaning

df = df.replace([7, 8, 9], pd.NA) # droping (replacing with "NaN") invalid survey-type values 

df = df[df["DIQ010"].isin([1, 2])] # drop any value in column DIQ010 (was diabetes diagnosed) that's not 1 or 2 (yes/no)
df["diabetes"] = df["DIQ10"].replace({1:1, 2:0}) # convert to binary - diabetes positive = 1, negative = 0

df = df[df["RIDAGEYR"] >= 20]

df["SBP_mean"] = df[["BPXSY1", "BPXSY2", "BPXSY3"]].mean(axis=1) # Systolic Blood Pressure - mean out of 3 samples (in NHANES examinations its examined 3 times per participant) 
df["DBP_mean"] = df[["BPXDI1", "BPXDI2", "BPXDI3"]].mean(axis=1) # Diastolic Blood Pressure - mean out of 3 samples (in NHANES examinations its examined 3 times per participant

# eliminating any potential tail values - borderline insane readings (normalizing the scale of measurementrs)
df = df[
    (df["BMXBMI"].between(10, 80)) &
    (df["SBP_mean"].between(70, 250)) &
    (df["DBP_mean"].between(40, 150))
]

fundamental_features = [                                
    "RIDAGEYR", "RIAGENDR", "RIDRETH1",
    "DMDEDUC2", "INDHHIN2",
    "BMXBMI", "BMXWAIST",
    "SBP_mean", "DBP_mean",

    "LBXSCH", "LBXSTR", #lipids

    "PAQ605", "SMQ020", "ALQ101", #habits

    "DR1TKCAL", "DR1TSUGR", #diet
    
    "SLD010H" #sleep
]

categorical_columns = ["RIAGENDR", "RIDRETH1", "DMDEDUC2", "INDHHIN2"] # columns with categories encoded as integers changed to strings, so model doesn't treat them like numbers (bigger/smaller)

# -------------------------- RISK PREDICTION -------------------

risk_features = fundamental_features.copy()

X_risk = df[risk_features]
y = df["diabetes"] # <--- target value

Xr_train, Xr_test, yr_train, yr_test = train_test_split(
    X_risk, y, test_size=0.2, stratify=y, random_state=42
)

label_encoders = {}

for col in categorical_columns:
    label = LabelEncoder()
    Xr_train[col] = label.fit_transform(Xr_train[col].astype(str))
    Xr_test[col] = label.fit_transform(Xr_test[col].astype(str))
    label_encoders[col] = label

# --------------------------- CLINICAL --------------------
# leave it for now
# clinical_features = risk_features + ["LBXSIR", "LBXGH", "LBXSGL"] # lipid and/or glucose based??
# X_clinical = df[clinical_features]
# ----------------------------------------------------------

xgb_risk = xgb.XGBClassifier(
    n_estimators=303,
    max_depth=5,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    eval_metric="logloss",
    random_state=42
)
